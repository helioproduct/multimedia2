{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "SE719lgjKPfo",
        "outputId": "71fca10f-4f33-4b8c-9f00-2ba0d0c37d1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m118.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m90.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m53.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m87.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing data.yaml at: /kaggle/input/fish-detection/data.yaml\n",
            "Creating new Ultralytics Settings v0.0.6 file ✅ \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8n.pt to 'yolov8n.pt'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6.25M/6.25M [00:00<00:00, 120MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics 8.3.134 🚀 Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/kaggle/input/fish-detection/data.yaml, degrees=0.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=10, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.001, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=fish_baseline, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=runs/detect/fish_baseline, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
            "Downloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 755k/755k [00:00<00:00, 25.8MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding model.yaml nc=80 with nc=13\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
            " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
            " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 22        [15, 18, 21]  1    753847  ultralytics.nn.modules.head.Detect           [13, [64, 128, 256]]          \n",
            "Model summary: 129 layers, 3,013,383 parameters, 3,013,367 gradients, 8.2 GFLOPs\n",
            "\n",
            "Transferred 319/355 items from pretrained weights\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n.pt to 'yolo11n.pt'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5.35M/5.35M [00:00<00:00, 110MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 11.7±7.2 MB/s, size: 46.5 KB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /kaggle/input/fish-detection/train/labels... 6842 images, 0 backgrounds, 0 corrupt: 100%|██████████| 6842/6842 [00:36<00:00, 187.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING ⚠️ \u001b[34m\u001b[1mtrain: \u001b[0mCache directory /kaggle/input/fish-detection/train is not writeable, cache not saved.\n",
            "WARNING ⚠️ Box and segment counts should be equal, but got len(segments) = 536, len(boxes) = 14154. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 14.7±7.9 MB/s, size: 37.0 KB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/input/fish-detection/valid/labels... 700 images, 0 backgrounds, 0 corrupt: 100%|██████████| 700/700 [00:03<00:00, 184.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING ⚠️ \u001b[34m\u001b[1mval: \u001b[0mCache directory /kaggle/input/fish-detection/valid is not writeable, cache not saved.\n",
            "WARNING ⚠️ Box and segment counts should be equal, but got len(segments) = 84, len(boxes) = 1197. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n",
            "Plotting labels to runs/detect/fish_baseline/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.001' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000588, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
            "Image sizes 640 train, 640 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1mruns/detect/fish_baseline\u001b[0m\n",
            "Starting training for 10 epochs...\n",
            "Closing dataloader mosaic\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       1/10      2.26G      1.343      3.115      1.614         13        640: 100%|██████████| 428/428 [02:11<00:00,  3.25it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 22/22 [00:09<00:00,  2.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        700       1197      0.656      0.547      0.639      0.406\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       2/10      2.66G      1.324      2.083      1.541         15        640: 100%|██████████| 428/428 [02:04<00:00,  3.44it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 22/22 [00:07<00:00,  2.89it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        700       1197      0.723      0.636      0.706      0.418\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       3/10      2.68G      1.272      1.669      1.495         25        640: 100%|██████████| 428/428 [02:04<00:00,  3.44it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 22/22 [00:07<00:00,  2.91it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        700       1197      0.689      0.674       0.73      0.466\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       4/10       2.7G      1.242      1.389       1.46         14        640: 100%|██████████| 428/428 [02:04<00:00,  3.44it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 22/22 [00:07<00:00,  2.95it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        700       1197      0.742      0.702      0.793      0.531\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       5/10      2.71G      1.192      1.188      1.413         19        640: 100%|██████████| 428/428 [02:04<00:00,  3.45it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 22/22 [00:07<00:00,  2.95it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        700       1197      0.752      0.792      0.822       0.55\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       6/10      2.73G      1.147      1.051      1.376         23        640: 100%|██████████| 428/428 [02:03<00:00,  3.47it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 22/22 [00:07<00:00,  2.97it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        700       1197      0.847      0.822      0.875      0.601\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       7/10      2.75G        1.1     0.9456      1.349         11        640: 100%|██████████| 428/428 [02:06<00:00,  3.37it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 22/22 [00:07<00:00,  2.99it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        700       1197      0.856       0.82      0.889      0.628\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       8/10      2.76G      1.061     0.8579      1.311         24        640: 100%|██████████| 428/428 [02:04<00:00,  3.44it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 22/22 [00:07<00:00,  2.96it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        700       1197      0.866       0.82      0.885      0.636\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       9/10      2.78G      1.014     0.7768      1.277         11        640: 100%|██████████| 428/428 [02:06<00:00,  3.40it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 22/22 [00:07<00:00,  2.98it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        700       1197      0.888      0.853      0.908       0.65\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      10/10       2.8G     0.9878     0.7319      1.254         34        640: 100%|██████████| 428/428 [02:04<00:00,  3.44it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 22/22 [00:07<00:00,  2.93it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        700       1197      0.872      0.874      0.913      0.667\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "10 epochs completed in 0.371 hours.\n",
            "Optimizer stripped from runs/detect/fish_baseline/weights/last.pt, 6.2MB\n",
            "Optimizer stripped from runs/detect/fish_baseline/weights/best.pt, 6.2MB\n",
            "\n",
            "Validating runs/detect/fish_baseline/weights/best.pt...\n",
            "Ultralytics 8.3.134 🚀 Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
            "Model summary (fused): 72 layers, 3,008,183 parameters, 0 gradients, 8.1 GFLOPs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 22/22 [00:10<00:00,  2.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        700       1197      0.871      0.875      0.912      0.667\n",
            "              BlueTang          7         23      0.872      0.913      0.902      0.812\n",
            "         ButterflyFish         10         47      0.846      0.894        0.9      0.483\n",
            "             ClownFish         28         41      0.877      0.872      0.941       0.64\n",
            "              GoldFish         62        134      0.845      0.937      0.934      0.658\n",
            "               Gourami        188        210      0.968          1      0.995       0.91\n",
            "            MorishIdol         23         31      0.936      0.942      0.986      0.676\n",
            "             PlatyFish         50        102      0.786      0.755      0.835      0.594\n",
            "     RibbonedSweetlips         22         67      0.798      0.836      0.876      0.655\n",
            "ThreeStripedDamselfish         79        108      0.962       0.95      0.989      0.797\n",
            "         YellowCichlid         22         95      0.832      0.484      0.648      0.362\n",
            "            YellowTang         45         80      0.785      0.925      0.949      0.604\n",
            "             ZebraFish        164        259      0.948      0.987      0.993      0.815\n",
            "Speed: 0.3ms preprocess, 2.4ms inference, 0.0ms loss, 3.6ms postprocess per image\n",
            "Results saved to \u001b[1mruns/detect/fish_baseline\u001b[0m\n",
            "\n",
            "📊 Baseline metrics:\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'DetMetrics' object has no attribute 'metrics'. See valid attributes below.\n\n    Utility class for computing detection metrics such as precision, recall, and mean average precision (mAP).\n\n    Attributes:\n        save_dir (Path): A path to the directory where the output plots will be saved.\n        plot (bool): A flag that indicates whether to plot precision-recall curves for each class.\n        names (dict): A dictionary of class names.\n        box (Metric): An instance of the Metric class for storing detection results.\n        speed (dict): A dictionary for storing execution times of different parts of the detection process.\n        task (str): The task type, set to 'detect'.\n    ",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-9addc0a09177>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# 4) Выводим ключевые метрики\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n📊 Baseline metrics:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvals\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults_baseline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{metric}: {vals[-1]:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ultralytics/utils/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0;34m\"\"\"Custom attribute access error message with helpful information.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"'{name}' object has no attribute '{attr}'. See valid attributes below.\\n{self.__doc__}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'DetMetrics' object has no attribute 'metrics'. See valid attributes below.\n\n    Utility class for computing detection metrics such as precision, recall, and mean average precision (mAP).\n\n    Attributes:\n        save_dir (Path): A path to the directory where the output plots will be saved.\n        plot (bool): A flag that indicates whether to plot precision-recall curves for each class.\n        names (dict): A dictionary of class names.\n        box (Metric): An instance of the Metric class for storing detection results.\n        speed (dict): A dictionary for storing execution times of different parts of the detection process.\n        task (str): The task type, set to 'detect'.\n    "
          ]
        }
      ],
      "source": [
        "!pip install ultralytics -q\n",
        "\n",
        "import kagglehub, os\n",
        "dataset_str = kagglehub.dataset_download(\"zehraatlgan/fish-detection\")\n",
        "data_yaml = os.path.join(dataset_str, \"data.yaml\")\n",
        "print(\"Using data.yaml at:\", data_yaml)\n",
        "\n",
        "from ultralytics import YOLO\n",
        "\n",
        "# Бейзлайн\n",
        "model = YOLO('yolov8n.pt')\n",
        "results_baseline = model.train(\n",
        "    data=data_yaml,\n",
        "    epochs=10,\n",
        "    imgsz=640,\n",
        "    batch=16,\n",
        "    lr0=1e-3,\n",
        "    name='fish_baseline'\n",
        ")\n",
        "\n",
        "print(\"\\n Baseline metrics:\")\n",
        "for metric, vals in results_baseline.metrics.items():\n",
        "    print(f\"{metric}: {vals[-1]:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "metrics = results_baseline.results_dict\n",
        "\n",
        "for name, value in metrics.items():\n",
        "    print(f\"{name}: {value:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NfUnc422OwIF",
        "outputId": "7d829161-9d6d-40f6-f383-ddca4315aaa4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "metrics/precision(B): 0.8714\n",
            "metrics/recall(B): 0.8746\n",
            "metrics/mAP50(B): 0.9124\n",
            "metrics/mAP50-95(B): 0.6671\n",
            "fitness: 0.6916\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EqE9hVJgRNy8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Для повышения mAP мы провели 12-эпоховую дообучение YOLOv8n с агрессивными аугментациями (RandAugment, Copy-Paste, MixUp и Mosaic), оптимизатором AdamW и cosine-LR scheduler, чтобы разнообразить обучающие примеры и сгладить процесс обучения, что улучшает обобщающую способность модели.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TCfVaAPbR4QJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = YOLO('yolov8n.pt')\n",
        "\n",
        "# 3) Тренировка: 12 эпох, агрессивные аугментации, AdamW + cosine LR\n",
        "results = model.train(\n",
        "    data=data_yaml,\n",
        "    epochs=12,\n",
        "    imgsz=640,\n",
        "    batch=16,\n",
        "    lr0=5e-4,\n",
        "    optimizer='AdamW',\n",
        "    cos_lr=True,\n",
        "    augment=True,\n",
        "    auto_augment='v0',\n",
        "    copy_paste=0.2,\n",
        "    mixup=0.2,\n",
        "    mosaic=0.5,\n",
        "    fliplr=0.5,\n",
        "    flipud=0.3,\n",
        "    hsv_h=0.02, hsv_s=0.8, hsv_v=0.5,\n",
        "    name='fish_augmented_12ep'\n",
        ")\n",
        "\n",
        "metrics = results.results_dict\n",
        "print(\"\\n Metrics after 12 epochs on augmented data:\")\n",
        "print(f\"Precision:    {metrics['precision']:.4f}\")\n",
        "print(f\"Recall:       {metrics['recall']:.4f}\")\n",
        "print(f\"mAP@50:       {metrics['map50']:.4f}\")\n",
        "print(f\"mAP@50-95:    {metrics['map50_95']:.4f}\")\n",
        "print(f\"Fitness:      {metrics.get('fitness', float('nan')):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "W13BNqcBR5kT",
        "outputId": "bcb927ec-c890-48b5-bdcc-dcf810bcd90b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics 8.3.134 🚀 Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=True, auto_augment=v0, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.2, copy_paste_mode=flip, cos_lr=True, cutmix=0.0, data=/kaggle/input/fish-detection/data.yaml, degrees=0.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=12, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.3, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.02, hsv_s=0.8, hsv_v=0.5, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.0005, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.2, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=0.5, multi_scale=False, name=fish_augmented_12ep, nbs=64, nms=False, opset=None, optimize=False, optimizer=AdamW, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=runs/detect/fish_augmented_12ep, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
            "Overriding model.yaml nc=80 with nc=13\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
            " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
            " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 22        [15, 18, 21]  1    753847  ultralytics.nn.modules.head.Detect           [13, [64, 128, 256]]          \n",
            "Model summary: 129 layers, 3,013,383 parameters, 3,013,367 gradients, 8.2 GFLOPs\n",
            "\n",
            "Transferred 319/355 items from pretrained weights\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ✅ (ping: 0.1±0.1 ms, read: 78.1±17.4 MB/s, size: 46.5 KB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /kaggle/input/fish-detection/train/labels... 6842 images, 0 backgrounds, 0 corrupt: 100%|██████████| 6842/6842 [00:11<00:00, 570.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING ⚠️ \u001b[34m\u001b[1mtrain: \u001b[0mCache directory /kaggle/input/fish-detection/train is not writeable, cache not saved.\n",
            "WARNING ⚠️ Box and segment counts should be equal, but got len(segments) = 536, len(boxes) = 14154. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.2±0.2 ms, read: 101.0±39.2 MB/s, size: 37.0 KB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/input/fish-detection/valid/labels... 700 images, 0 backgrounds, 0 corrupt: 100%|██████████| 700/700 [00:01<00:00, 498.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING ⚠️ \u001b[34m\u001b[1mval: \u001b[0mCache directory /kaggle/input/fish-detection/valid is not writeable, cache not saved.\n",
            "WARNING ⚠️ Box and segment counts should be equal, but got len(segments) = 84, len(boxes) = 1197. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Plotting labels to runs/detect/fish_augmented_12ep/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.0005, momentum=0.937) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
            "Image sizes 640 train, 640 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1mruns/detect/fish_augmented_12ep\u001b[0m\n",
            "Starting training for 12 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       1/12      2.71G      1.559      2.876      1.709         27        640: 100%|██████████| 428/428 [02:36<00:00,  2.74it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 22/22 [00:07<00:00,  2.91it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        700       1197      0.513       0.51      0.494       0.31\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       2/12      2.98G      1.512      2.075      1.653         46        640: 100%|██████████| 428/428 [02:28<00:00,  2.88it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 22/22 [00:06<00:00,  3.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        700       1197       0.55       0.57      0.589      0.366\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Closing dataloader mosaic\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       3/12      2.98G      1.335      1.512      1.585         28        640: 100%|██████████| 428/428 [02:07<00:00,  3.35it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 22/22 [00:06<00:00,  3.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        700       1197      0.627      0.715      0.724      0.446\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       4/12      2.98G      1.274      1.301      1.509         14        640: 100%|██████████| 428/428 [02:08<00:00,  3.33it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 22/22 [00:06<00:00,  3.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        700       1197       0.68       0.73       0.74       0.48\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       5/12         3G      1.225      1.166       1.47         19        640: 100%|██████████| 428/428 [02:07<00:00,  3.37it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 22/22 [00:07<00:00,  3.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        700       1197      0.762      0.724      0.784      0.522\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       6/12      3.01G      1.197      1.066      1.439         23        640: 100%|██████████| 428/428 [02:07<00:00,  3.36it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 22/22 [00:07<00:00,  3.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        700       1197      0.735      0.785      0.817       0.56\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       7/12      3.03G      1.147     0.9773      1.412         11        640: 100%|██████████| 428/428 [02:04<00:00,  3.45it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 22/22 [00:07<00:00,  2.96it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        700       1197      0.826        0.8      0.857      0.601\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       8/12      3.05G      1.109     0.9045      1.375         24        640: 100%|██████████| 428/428 [02:06<00:00,  3.37it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 22/22 [00:07<00:00,  3.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        700       1197      0.841      0.826      0.885       0.62\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       9/12      3.06G      1.073     0.8271      1.339         11        640: 100%|██████████| 428/428 [02:07<00:00,  3.36it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 22/22 [00:07<00:00,  3.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        700       1197      0.851      0.841       0.88      0.639\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      10/12      3.08G      1.038     0.7729      1.322         34        640: 100%|██████████| 428/428 [02:04<00:00,  3.43it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 22/22 [00:07<00:00,  3.12it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        700       1197      0.845      0.878      0.902      0.653\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      11/12       3.1G      1.017     0.7456      1.298         21        640: 100%|██████████| 428/428 [02:04<00:00,  3.43it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 22/22 [00:08<00:00,  2.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        700       1197      0.845      0.889      0.903      0.664\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      12/12      3.12G      1.008     0.7289      1.296         24        640: 100%|██████████| 428/428 [02:05<00:00,  3.42it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 22/22 [00:07<00:00,  3.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        700       1197      0.839      0.886        0.9      0.665\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "12 epochs completed in 0.461 hours.\n",
            "Optimizer stripped from runs/detect/fish_augmented_12ep/weights/last.pt, 6.2MB\n",
            "Optimizer stripped from runs/detect/fish_augmented_12ep/weights/best.pt, 6.2MB\n",
            "\n",
            "Validating runs/detect/fish_augmented_12ep/weights/best.pt...\n",
            "Ultralytics 8.3.134 🚀 Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
            "Model summary (fused): 72 layers, 3,008,183 parameters, 0 gradients, 8.1 GFLOPs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 22/22 [00:10<00:00,  2.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        700       1197      0.839      0.873      0.909      0.676\n",
            "              BlueTang          7         23      0.869      0.864      0.922      0.831\n",
            "         ButterflyFish         10         47      0.736      0.809       0.81      0.452\n",
            "             ClownFish         28         41      0.845      0.976      0.946      0.658\n",
            "              GoldFish         62        134      0.795      0.924      0.939      0.694\n",
            "               Gourami        188        210      0.929          1      0.995      0.915\n",
            "            MorishIdol         23         31      0.876      0.903      0.967      0.767\n",
            "             PlatyFish         50        102      0.768      0.784      0.864      0.605\n",
            "     RibbonedSweetlips         22         67      0.758       0.91      0.899      0.646\n",
            "ThreeStripedDamselfish         79        108      0.895       0.95      0.979      0.771\n",
            "         YellowCichlid         22         95      0.895      0.449      0.684      0.368\n",
            "            YellowTang         45         80       0.75      0.912      0.908      0.599\n",
            "             ZebraFish        164        259      0.956      0.996      0.995      0.808\n",
            "Speed: 0.3ms preprocess, 7.6ms inference, 0.0ms loss, 2.2ms postprocess per image\n",
            "Results saved to \u001b[1mruns/detect/fish_augmented_12ep\u001b[0m\n",
            "\n",
            "📊 Metrics after 12 epochs on augmented data:\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'precision'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-9cfe85f255b1>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n📊 Metrics after 12 epochs on augmented data:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Precision:    {metrics['precision']:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Recall:       {metrics['recall']:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"mAP@50:       {metrics['map50']:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'precision'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "metrics = results.results_dict\n",
        "\n",
        "for name, value in metrics.items():\n",
        "    print(f\"{name}: {value:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZ4XqQUqYrDC",
        "outputId": "525a7032-e22f-4ecb-eaea-371c9bb20abc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "metrics/precision(B): 0.8393\n",
            "metrics/recall(B): 0.8731\n",
            "metrics/mAP50(B): 0.9091\n",
            "metrics/mAP50-95(B): 0.6762\n",
            "fitness: 0.6995\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Судя по результатам, агрессивные аугментации вместе с AdamW + cosine дали небольшой прирост mAP50-95 (0.6762 vs 0.6671), но заметно снизили precision (0.8393 vs 0.8714) и слегка упали mAP50 (0.9091 vs 0.9124) — то есть модель лучше научилась «точно» локализовывать объекты по IoU, но стала чаще делать ложные срабатывания."
      ],
      "metadata": {
        "id": "2Y5crgeWY4-i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "собственная имплементация"
      ],
      "metadata": {
        "id": "YlYT97CCZMAg"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9-g2hGo0ZNp-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import yaml\n",
        "import glob\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as T\n",
        "import kagglehub\n",
        "\n",
        "# 1. Download dataset and get path\n",
        "dataset_str = kagglehub.dataset_download(\"zehraatlgan/fish-detection\")\n",
        "data_yaml = os.path.join(dataset_str, \"data.yaml\")\n",
        "print(\"Using data.yaml at:\", data_yaml)\n",
        "\n",
        "# 2. Load dataset configuration\n",
        "with open(data_yaml, 'r') as f:\n",
        "    data_cfg = yaml.safe_load(f)\n",
        "\n",
        "# 3. Resolve image directories (handle various relative paths)\n",
        "base_dir = os.path.dirname(data_yaml)\n",
        "train_candidates = [\n",
        "    os.path.normpath(os.path.join(base_dir, data_cfg['train'])),\n",
        "    os.path.normpath(os.path.join(dataset_str, data_cfg['train'])),\n",
        "    os.path.join(dataset_str, 'train/images'),\n",
        "    os.path.join(dataset_str, 'train'),\n",
        "]\n",
        "val_candidates = [\n",
        "    os.path.normpath(os.path.join(base_dir, data_cfg['val'])),\n",
        "    os.path.normpath(os.path.join(dataset_str, data_cfg['val'])),\n",
        "    os.path.join(dataset_str, 'valid/images'),\n",
        "    os.path.join(dataset_str, 'valid'),\n",
        "    os.path.join(dataset_str, 'val/images'),\n",
        "    os.path.join(dataset_str, 'val'),\n",
        "]\n",
        "\n",
        "# Select first existing directory\n",
        "train_img_dir = next((d for d in train_candidates if os.path.isdir(d)), None)\n",
        "val_img_dir   = next((d for d in val_candidates   if os.path.isdir(d)), None)\n",
        "if train_img_dir is None:\n",
        "    raise FileNotFoundError(f\"No valid train directory found among: {train_candidates}\")\n",
        "if val_img_dir is None:\n",
        "    raise FileNotFoundError(f\"No valid val directory found among: {val_candidates}\")\n",
        "\n",
        "print(f\"Using train images from: {train_img_dir}\")\n",
        "print(f\"Using val images from:   {val_img_dir}\")\n",
        "\n",
        "# Class names\n",
        "class_names = data_cfg.get('names', {})\n",
        "num_classes = len(class_names)\n",
        "\n",
        "# 4. Prepare small subset\n",
        "train_imgs = sorted(glob.glob(os.path.join(train_img_dir, '*.jpg')))[:100]\n",
        "val_imgs   = sorted(glob.glob(os.path.join(val_img_dir,   '*.jpg')))[:50]\n",
        "if not train_imgs:\n",
        "    raise RuntimeError(f\"No JPG images found in train dir: {train_img_dir}\")\n",
        "if not val_imgs:\n",
        "    raise RuntimeError(f\"No JPG images found in val dir: {val_img_dir}\")\n",
        "\n",
        "# 5. Dataset class\n",
        "class FishDataset(Dataset):\n",
        "    def __init__(self, img_paths, img_size=640, grid_size=20):\n",
        "        self.img_paths = img_paths\n",
        "        self.img_size  = img_size\n",
        "        self.grid_size = grid_size\n",
        "        self.transform = T.Compose([\n",
        "            T.Resize((img_size, img_size)),\n",
        "            T.ToTensor(),\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.img_paths[idx]\n",
        "        txt_path = img_path.replace('.jpg', '.txt')\n",
        "\n",
        "        img = Image.open(img_path).convert('RGB')\n",
        "        img = self.transform(img)\n",
        "\n",
        "        S = self.grid_size\n",
        "        target = torch.zeros((5, S, S))\n",
        "\n",
        "        if os.path.isfile(txt_path):\n",
        "            for line in open(txt_path):\n",
        "                cls, x, y, w, h = map(float, line.split())\n",
        "                gx, gy = x * S, y * S\n",
        "                gi, gj = int(gx), int(gy)\n",
        "                tx, ty = gx - gi, gy - gj\n",
        "                tw, th = w * S, h * S\n",
        "                target[0, gj, gi] = tx\n",
        "                target[1, gj, gi] = ty\n",
        "                target[2, gj, gi] = tw\n",
        "                target[3, gj, gi] = th\n",
        "                target[4, gj, gi] = 1.0\n",
        "\n",
        "        return img, target\n",
        "\n",
        "def collate_fn(batch):\n",
        "    imgs, targets = zip(*batch)\n",
        "    return torch.stack(imgs), torch.stack(targets)\n",
        "\n",
        "# 6. Model definition\n",
        "class SimpleYOLO(nn.Module):\n",
        "    def __init__(self, grid_size=20):\n",
        "        super().__init__()\n",
        "        self.backbone = nn.Sequential(\n",
        "            nn.Conv2d(3, 16, 3, 1, 1), nn.ReLU(), nn.MaxPool2d(2),\n",
        "            nn.Conv2d(16, 32, 3, 1, 1), nn.ReLU(), nn.MaxPool2d(2),\n",
        "            nn.Conv2d(32, 64, 3, 1, 1), nn.ReLU(), nn.MaxPool2d(2),\n",
        "            nn.Conv2d(64, 128,3, 1, 1), nn.ReLU(), nn.MaxPool2d(2),\n",
        "            nn.Conv2d(128,256,3, 1, 1), nn.ReLU(), nn.MaxPool2d(2)\n",
        "        )\n",
        "        self.pred = nn.Conv2d(256, 5, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.pred(self.backbone(x))\n",
        "\n",
        "# 7. Setup data loaders\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "train_ds = FishDataset(train_imgs)\n",
        "train_dl = DataLoader(train_ds, batch_size=8, shuffle=True, collate_fn=collate_fn)\n",
        "val_ds   = FishDataset(val_imgs)\n",
        "val_dl   = DataLoader(val_ds,   batch_size=8, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "# 8. Training loop\n",
        "def train_one_epoch(model, dataloader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    for imgs, targets in dataloader:\n",
        "        imgs, targets = imgs.to(device), targets.to(device)\n",
        "        preds = model(imgs)\n",
        "        loss  = criterion(preds, targets)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward() # ебанутые блять\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "model     = SimpleYOLO().to(device)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=5e-4)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "epochs = 5\n",
        "for epoch in range(1, epochs+1):\n",
        "    loss = train_one_epoch(model, train_dl, optimizer, criterion, device)\n",
        "    print(f\"Epoch {epoch}/{epochs} - Loss: {loss:.4f}\")\n",
        "\n",
        "# 9. Save\n",
        "torch.save(model.state_dict(), 'simple_yolo_fish.pth')\n",
        "print(\"Training complete. Model saved to simple_yolo_fish.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "drrLtUvLZjnJ",
        "outputId": "4664d35d-2340-4da9-949a-078b70ddecec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using data.yaml at: /kaggle/input/fish-detection/data.yaml\n",
            "Using train images from: /kaggle/input/fish-detection/train/images\n",
            "Using val images from:   /kaggle/input/fish-detection/valid/images\n",
            "Epoch 1/5 - Loss: 0.0002\n",
            "Epoch 2/5 - Loss: 0.0000\n",
            "Epoch 3/5 - Loss: 0.0000\n",
            "Epoch 4/5 - Loss: 0.0000\n",
            "Epoch 5/5 - Loss: 0.0000\n",
            "Training complete. Model saved to simple_yolo_fish.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = SimpleYOLO().to(device)\n",
        "model.load_state_dict(torch.load('simple_yolo_fish.pth'))\n",
        "model.eval()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZfyNqqzMdeP4",
        "outputId": "8e6be5e6-aad5-4805-9eae-b4f2b566f5ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SimpleYOLO(\n",
              "  (backbone): Sequential(\n",
              "    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU()\n",
              "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (3): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (4): ReLU()\n",
              "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (6): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (7): ReLU()\n",
              "    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (9): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (10): ReLU()\n",
              "    (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (12): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (13): ReLU()\n",
              "    (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (pred): Conv2d(256, 5, kernel_size=(1, 1), stride=(1, 1))\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision.ops import box_iou\n",
        "\n",
        "def decode_preds(pred, S=20, img_size=640, conf_thr=0.5):\n",
        "    \"\"\"\n",
        "    Находит в каждом батче все ячейки с conf>conf_thr\n",
        "    и превращает их в боксы (x1,y1,x2,y2) в пикселях.\n",
        "    \"\"\"\n",
        "    B = pred.size(0)\n",
        "    cell = img_size / S\n",
        "    all_preds = []\n",
        "    for b in range(B):\n",
        "        p = pred[b]  # [5,S,S]\n",
        "        objs = p[4]\n",
        "        idxs = (objs > conf_thr).nonzero(as_tuple=False)\n",
        "        boxes, scores = [], []\n",
        "        for gj, gi in idxs:\n",
        "            tx, ty, tw, th = p[0,gj,gi], p[1,gj,gi], p[2,gj,gi], p[3,gj,gi]\n",
        "            cx = (gi + tx) * cell\n",
        "            cy = (gj + ty) * cell\n",
        "            w  = tw * cell\n",
        "            h  = th * cell\n",
        "            x1, y1 = cx - w/2, cy - h/2\n",
        "            x2, y2 = cx + w/2, cy + h/2\n",
        "            boxes.append([x1,y1,x2,y2])\n",
        "            scores.append(objs[gj,gi].item())\n",
        "        if boxes:\n",
        "            all_preds.append({\n",
        "                'boxes': torch.tensor(boxes),\n",
        "                'scores': torch.tensor(scores)\n",
        "            })\n",
        "        else:\n",
        "            all_preds.append({'boxes': torch.zeros((0,4)), 'scores': torch.zeros((0,))})\n",
        "    return all_preds\n",
        "\n",
        "def get_gt_boxes(target, S=20, img_size=640):\n",
        "    \"\"\"\n",
        "    Из таргета [5,S,S] возвращает тензор [N,4] истинных боксов.\n",
        "    \"\"\"\n",
        "    cell = img_size / S\n",
        "    obj = target[4]\n",
        "    idxs = (obj > 0.5).nonzero(as_tuple=False)\n",
        "    boxes = []\n",
        "    for gj, gi in idxs:\n",
        "        tx, ty, tw, th = target[0,gj,gi], target[1,gj,gi], target[2,gj,gi], target[3,gj,gi]\n",
        "        cx = (gi + tx) * cell\n",
        "        cy = (gj + ty) * cell\n",
        "        w  = tw * cell\n",
        "        h  = th * cell\n",
        "        x1, y1 = cx - w/2, cy - h/2\n",
        "        x2, y2 = cx + w/2, cy + h/2\n",
        "        boxes.append([x1,y1,x2,y2])\n",
        "    return torch.tensor(boxes)\n"
      ],
      "metadata": {
        "id": "v6d8CtK0djxj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_tp = all_fp = all_fn = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for imgs, targets in val_dl:\n",
        "        imgs = imgs.to(device)\n",
        "        preds = model(imgs).cpu()\n",
        "        batch_preds = decode_preds(preds, S=20, img_size=640, conf_thr=0.5)\n",
        "\n",
        "        for i in range(len(batch_preds)):\n",
        "            pred = batch_preds[i]\n",
        "            gt   = get_gt_boxes(targets[i], S=20, img_size=640)\n",
        "\n",
        "            if len(pred['boxes']) == 0:\n",
        "                all_fn += len(gt); continue\n",
        "            if len(gt) == 0:\n",
        "                all_fp += len(pred['boxes']); continue\n",
        "\n",
        "            ious = box_iou(pred['boxes'], gt)\n",
        "            # для каждого предикта берём максимальный IoU\n",
        "            max_iou, _ = ious.max(dim=1)  # [P]\n",
        "            tp = (max_iou >= 0.5).sum().item()\n",
        "            fp = (max_iou < 0.5).sum().item()\n",
        "            fn = len(gt) - tp\n",
        "\n",
        "            all_tp += tp\n",
        "            all_fp += fp\n",
        "            all_fn += fn\n",
        "\n",
        "precision = all_tp / (all_tp + all_fp + 1e-8)\n",
        "recall    = all_tp / (all_tp + all_fn + 1e-8)\n",
        "f1        = 2 * precision * recall / (precision + recall + 1e-8)\n",
        "\n",
        "print(f\"Val — Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m3sengUCdnHu",
        "outputId": "f755a267-f760-403a-fc4c-ad1c1d148a8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val — Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import yaml\n",
        "import glob\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as T\n",
        "import kagglehub\n",
        "from torchvision.ops import box_iou\n",
        "\n",
        "# =========================\n",
        "# 1. DATA LOADING\n",
        "# =========================\n",
        "print(\"Downloading dataset (cached after first run)…\")\n",
        "root = kagglehub.dataset_download(\"zehraatlgan/fish-detection\")\n",
        "ctx_yaml = os.path.join(root, \"data.yaml\")\n",
        "print(\"Using data.yaml:\", ctx_yaml)\n",
        "\n",
        "with open(ctx_yaml) as f:\n",
        "    cfg = yaml.safe_load(f)\n",
        "train_dir = os.path.normpath(os.path.join(root, cfg[\"train\"].lstrip(\"../\")))\n",
        "val_dir   = os.path.normpath(os.path.join(root, cfg[\"val\"].lstrip(\"../\")))\n",
        "print(\"Train dir:\", train_dir)\n",
        "print(\"Val dir  :\", val_dir)\n",
        "\n",
        "train_imgs = sorted(glob.glob(os.path.join(train_dir, \"*.jpg\")))[:128]\n",
        "val_imgs   = sorted(glob.glob(os.path.join(val_dir,   \"*.jpg\")))[:64]\n",
        "assert train_imgs and val_imgs, \"Images not found!\"\n",
        "IMG_SIZE = 416  # reduce resolution to cut memory\n",
        "\n",
        "# =========================\n",
        "# 2. DATASET\n",
        "# =========================\n",
        "class FishDataset(Dataset):\n",
        "    def __init__(self, paths, S=13, augment=False):\n",
        "        self.paths = paths; self.S=S\n",
        "        base = [T.Resize((IMG_SIZE, IMG_SIZE)), T.ToTensor()]\n",
        "        aug  = [T.RandomHorizontalFlip()] if augment else []\n",
        "        self.tf = T.Compose(aug+base)\n",
        "    def __len__(self): return len(self.paths)\n",
        "    def __getitem__(self, idx):\n",
        "        p = self.paths[idx]\n",
        "        img = self.tf(Image.open(p).convert('RGB'))\n",
        "        t = torch.zeros((5,self.S,self.S))\n",
        "        txt=p.replace('.jpg','.txt')\n",
        "        if os.path.exists(txt):\n",
        "            for l in open(txt):\n",
        "                _,x,y,w,h=map(float,l.split())\n",
        "                gx,gy=x*self.S,y*self.S; gi,gj=int(gx),int(gy)\n",
        "                t[0,gj,gi]=gx-gi; t[1,gj,gi]=gy-gj; t[2,gj,gi]=w*self.S; t[3,gj,gi]=h*self.S; t[4,gj,gi]=1.\n",
        "        return img,t\n",
        "\n",
        "def collate(b):\n",
        "    a,b=zip(*b); return torch.stack(a),torch.stack(b)\n",
        "\n",
        "# =========================\n",
        "# 3. MODEL (lighter)\n",
        "# =========================\n",
        "class TinyYOLO(nn.Module):\n",
        "    def __init__(self,S=13):\n",
        "        super().__init__()\n",
        "        def C(i,o): return nn.Sequential(nn.Conv2d(i,o,3,1,1), nn.BatchNorm2d(o), nn.LeakyReLU(0.1))\n",
        "        self.back = nn.Sequential(\n",
        "            C(3,16), nn.MaxPool2d(2),\n",
        "            C(16,32), nn.MaxPool2d(2),\n",
        "            C(32,64), nn.MaxPool2d(2),\n",
        "            C(64,128),nn.MaxPool2d(2),\n",
        "            C(128,256),nn.MaxPool2d(2)\n",
        "        )\n",
        "        self.pred = nn.Conv2d(256,5,1)\n",
        "        self.S=S\n",
        "    def forward(self,x): return self.pred(self.back(x))\n",
        "\n",
        "# =========================\n",
        "# 4. LOSS\n",
        "# =========================\n",
        "L1=nn.SmoothL1Loss()\n",
        "\n",
        "def loss_fn(p,t,λc=5,λo=1,λn=0.5,pos_w=15.):\n",
        "    obj=t[:,4:5]>0; bce=nn.BCEWithLogitsLoss(pos_weight=torch.tensor([pos_w],device=p.device))\n",
        "    lo=bce(p[:,4:5][obj],torch.ones_like(p[:,4:5][obj])) if obj.any() else torch.tensor(0.,device=p.device)\n",
        "    lno=bce(p[:,4:5][~obj],torch.zeros_like(p[:,4:5][~obj]))\n",
        "    lb=L1(p[:,:4][obj.expand_as(p[:,:4])],t[:,:4][obj.expand_as(t[:,:4])]) if obj.any() else torch.tensor(0.,device=p.device)\n",
        "    return λc*lb+λo*lo+λn*lno\n",
        "\n",
        "# =========================\n",
        "# 5. TRAIN\n",
        "# =========================\n",
        "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model=TinyYOLO().to(device)\n",
        "opt=optim.Adam(model.parameters(),lr=1e-3)\n",
        "train_dl=DataLoader(FishDataset(train_imgs,augment=True),batch_size=4,shuffle=True,collate_fn=collate)\n",
        "val_dl=DataLoader(FishDataset(val_imgs),batch_size=4,collate_fn=collate)\n",
        "\n",
        "epochs=10\n",
        "for ep in range(1,epochs+1):\n",
        "    model.train();run=0\n",
        "    for x,y in train_dl:\n",
        "        x,y=x.to(device),y.to(device)\n",
        "        out=model(x); loss=loss_fn(out,y)\n",
        "        opt.zero_grad(); loss.backward(); opt.step(); run+=loss.item()\n",
        "    torch.cuda.empty_cache()\n",
        "    print(f\"E{ep}/{epochs} loss {run/len(train_dl):.4f}\")\n",
        "\n",
        "# =========================\n",
        "# 6. VERY ROUGH VAL (counts any pred as TP)\n",
        "# =========================\n",
        "model.eval(); tp=len(val_imgs); fp=fn=0\n",
        "print(f\"Dummy Precision=1.0 Recall=1.0 (placeholder)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YBJbJiB-iJcg",
        "outputId": "61570b0b-ea8d-4dd3-ba23-eda0e62f9f68"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading dataset (cached after first run)…\n",
            "Resuming download from 128974848 bytes (229544954 bytes left)...\n",
            "Resuming download from https://www.kaggle.com/api/v1/datasets/download/zehraatlgan/fish-detection?dataset_version_number=1 (128974848/358519802) bytes left.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 342M/342M [00:10<00:00, 21.1MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using data.yaml: /root/.cache/kagglehub/datasets/zehraatlgan/fish-detection/versions/1/data.yaml\n",
            "Train dir: /root/.cache/kagglehub/datasets/zehraatlgan/fish-detection/versions/1/train/images\n",
            "Val dir  : /root/.cache/kagglehub/datasets/zehraatlgan/fish-detection/versions/1/valid/images\n",
            "E1/10 loss 0.0837\n",
            "E2/10 loss 0.0039\n",
            "E3/10 loss 0.0018\n",
            "E4/10 loss 0.0012\n",
            "E5/10 loss 0.0010\n",
            "E6/10 loss 0.0008\n",
            "E7/10 loss 0.0006\n",
            "E8/10 loss 0.0005\n",
            "E9/10 loss 0.0004\n",
            "E10/10 loss 0.0004\n",
            "Dummy Precision=1.0 Recall=1.0 (placeholder)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "j6AdqNTqjbq9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "js9_ZRhajeBk"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "as7yXh5kjbCT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, glob, yaml, torch, kagglehub\n",
        "import torch.nn as nn, torch.optim as optim\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as T\n",
        "from torchvision.ops import box_iou\n",
        "\n",
        "# =============== 1. DATA LOADING ===============\n",
        "print(\"⬇️  Downloading dataset (кешируется)…\")\n",
        "root = kagglehub.dataset_download(\"zehraatlgan/fish-detection\")\n",
        "yaml_path = os.path.join(root, \"data.yaml\")\n",
        "print(\"Using data.yaml:\", yaml_path)\n",
        "\n",
        "with open(yaml_path) as f:\n",
        "    cfg = yaml.safe_load(f)\n",
        "train_dir = os.path.normpath(os.path.join(root, cfg[\"train\"].lstrip(\"../\")))\n",
        "val_dir   = os.path.normpath(os.path.join(root, cfg[\"val\"].lstrip(\"../\")))\n",
        "print(\"Train dir:\", train_dir)\n",
        "print(\"Val dir  :\", val_dir)\n",
        "\n",
        "train_imgs = sorted(glob.glob(os.path.join(train_dir, \"*.jpg\")))[:256]\n",
        "val_imgs   = sorted(glob.glob(os.path.join(val_dir,   \"*.jpg\")))[:64]\n",
        "assert train_imgs and val_imgs, \"Images not found!\"\n",
        "\n",
        "IMG_SIZE = 416        # ↓ можно 320, если мало GPU-памяти\n",
        "GRID_S   = 13         # 416 / 32 ≈ 13\n",
        "\n",
        "# =============== 2. DATASET ===============\n",
        "class FishDataset(Dataset):\n",
        "    def __init__(self, paths, S=GRID_S, augment=False):\n",
        "        self.paths, self.S = paths, S\n",
        "        base = [T.Resize((IMG_SIZE, IMG_SIZE)), T.ToTensor()]\n",
        "        aug  = [T.ColorJitter(0.2,0.2,0.2,0.1),\n",
        "                T.RandomHorizontalFlip()] if augment else []\n",
        "        self.tf = T.Compose(aug + base)\n",
        "    def __len__(self): return len(self.paths)\n",
        "    def __getitem__(self, i):\n",
        "        p = self.paths[i]\n",
        "        img = self.tf(Image.open(p).convert(\"RGB\"))\n",
        "        t = torch.zeros((5, self.S, self.S))\n",
        "        lab = p.replace(\".jpg\", \".txt\")\n",
        "        if os.path.exists(lab):\n",
        "            for line in open(lab):\n",
        "                _,x,y,w,h = map(float, line.split())\n",
        "                gx, gy = x*self.S, y*self.S\n",
        "                gi, gj = int(gx), int(gy)\n",
        "                if gi>=self.S or gj>=self.S: continue\n",
        "                t[0,gj,gi] = gx-gi;  t[1,gj,gi] = gy-gj\n",
        "                t[2,gj,gi] = w*self.S; t[3,gj,gi] = h*self.S\n",
        "                t[4,gj,gi] = 1.\n",
        "        return img, t\n",
        "\n",
        "def collate(b): imgs,tg = zip(*b); return torch.stack(imgs), torch.stack(tg)\n",
        "\n",
        "# =============== 3. MODEL ===============\n",
        "class CBL(nn.Sequential):\n",
        "    def __init__(self, c_in, c_out, k=3, s=1):\n",
        "        super().__init__(\n",
        "            nn.Conv2d(c_in, c_out, k, s, k//2, bias=False),\n",
        "            nn.BatchNorm2d(c_out),\n",
        "            nn.LeakyReLU(0.1, inplace=True)\n",
        "        )\n",
        "\n",
        "class TinyYOLO(nn.Module):\n",
        "    def __init__(self, S=GRID_S):\n",
        "        super().__init__()\n",
        "        self.back = nn.Sequential(\n",
        "            CBL(3,16),  nn.MaxPool2d(2),\n",
        "            CBL(16,32), nn.MaxPool2d(2),\n",
        "            CBL(32,64), nn.MaxPool2d(2),\n",
        "            CBL(64,128),nn.MaxPool2d(2),\n",
        "            CBL(128,256),nn.MaxPool2d(2),\n",
        "            CBL(256,256)\n",
        "        )\n",
        "        self.pred = nn.Conv2d(256, 5, 1)   # tx,ty,tw,th,conf\n",
        "    def forward(self,x): return self.pred(self.back(x))\n",
        "\n",
        "# =============== 4. LOSS ===============\n",
        "smooth_l1 = nn.SmoothL1Loss(reduction='mean')\n",
        "def yolo_loss(p, t, λc=5, λo=1, λn=0.5, pos_w=15.):\n",
        "    obj = t[:,4:5] > 0\n",
        "    bce = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([pos_w], device=p.device))\n",
        "    lo  = bce(p[:,4:5][obj], torch.ones_like(p[:,4:5][obj])) if obj.any() else torch.tensor(0., device=p.device)\n",
        "    lno = bce(p[:,4:5][~obj], torch.zeros_like(p[:,4:5][~obj]))\n",
        "    lb  = smooth_l1(p[:,:4][obj.expand_as(p[:,:4])], t[:,:4][obj.expand_as(t[:,:4])]) if obj.any() else torch.tensor(0.,device=p.device)\n",
        "    return λc*lb + λo*lo + λn*lno\n",
        "\n",
        "# =============== 5. TRAINING ===============\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model  = TinyYOLO().to(device)\n",
        "opt    = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
        "\n",
        "train_dl = DataLoader(FishDataset(train_imgs, augment=True), batch_size=8, shuffle=True,  collate_fn=collate)\n",
        "val_dl   = DataLoader(FishDataset(val_imgs),               batch_size=8, shuffle=False, collate_fn=collate)\n",
        "\n",
        "print(\"🚀 Training…\")\n",
        "for epoch in range(1, 21):\n",
        "    model.train(); total=0\n",
        "    for imgs,tg in train_dl:\n",
        "        imgs, tg = imgs.to(device), tg.to(device)\n",
        "        loss = yolo_loss(model(imgs), tg)\n",
        "        opt.zero_grad(); loss.backward(); opt.step()\n",
        "        total += loss.item()\n",
        "    torch.cuda.empty_cache()\n",
        "    print(f\"Epoch {epoch:2d}/20 | loss {total/len(train_dl):.4f}\")\n",
        "\n",
        "# =============== 6. VALIDATION ===============\n",
        "print(\"🔍 Evaluating…\")\n",
        "model.eval(); tp=fp=fn=0\n",
        "cell = IMG_SIZE / GRID_S\n",
        "with torch.no_grad():\n",
        "    for imgs, tg in val_dl:\n",
        "        preds = model(imgs.to(device)).cpu()\n",
        "        for b in range(preds.size(0)):\n",
        "            p = preds[b]; conf = torch.sigmoid(p[4])\n",
        "            idx = (conf>0.25).nonzero(as_tuple=False)\n",
        "            pred_boxes = []\n",
        "            for gj,gi in idx:\n",
        "                tx,ty,tw,th = p[0,gj,gi], p[1,gj,gi], p[2,gj,gi], p[3,gj,gi]\n",
        "                cx = (gi+torch.sigmoid(tx))*cell\n",
        "                cy = (gj+torch.sigmoid(ty))*cell\n",
        "                w  = torch.exp(tw).clamp(max=5)*cell\n",
        "                h  = torch.exp(th).clamp(max=5)*cell\n",
        "                pred_boxes.append([cx-w/2, cy-h/2, cx+w/2, cy+h/2])\n",
        "            pred_boxes = torch.tensor(pred_boxes) if pred_boxes else torch.zeros((0,4))\n",
        "\n",
        "            gt = []\n",
        "            for gj,gi in (tg[b,4]>0).nonzero(as_tuple=False):\n",
        "                tx,ty,tw,th = tg[b,0,gj,gi], tg[b,1,gj,gi], tg[b,2,gj,gi], tg[b,3,gj,gi]\n",
        "                cx=(gi+tx)*cell; cy=(gj+ty)*cell; w=tw*cell; h=th*cell\n",
        "                gt.append([cx-w/2, cy-h/2, cx+w/2, cy+h/2])\n",
        "            gt = torch.tensor(gt) if gt else torch.zeros((0,4))\n",
        "\n",
        "            if pred_boxes.numel()==0: fn += len(gt); continue\n",
        "            if gt.numel()==0: fp += len(pred_boxes); continue\n",
        "            iou = box_iou(pred_boxes, gt)\n",
        "            m,_ = iou.max(1)\n",
        "            tp += (m>=0.5).sum().item()\n",
        "            fp += (m<0.5).sum().item()\n",
        "            fn += gt.size(0) - (m>=0.5).sum().item()\n",
        "\n",
        "prec = tp/(tp+fp+1e-8); rec = tp/(tp+fn+1e-8)\n",
        "f1   = 2*prec*rec/(prec+rec+1e-8)\n",
        "print(f\"✅ Precision={prec:.3f} Recall={rec:.3f} F1={f1:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pTK_JBkhjPpN",
        "outputId": "c3ac5e99-056e-47c7-fab2-da6d8079c2e7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⬇️  Downloading dataset (кешируется)…\n",
            "Using data.yaml: /kaggle/input/fish-detection/data.yaml\n",
            "Train dir: /kaggle/input/fish-detection/train/images\n",
            "Val dir  : /kaggle/input/fish-detection/valid/images\n",
            "🚀 Training…\n",
            "Epoch  1/20 | loss 0.1487\n",
            "Epoch  2/20 | loss 0.0208\n",
            "Epoch  3/20 | loss 0.0086\n",
            "Epoch  4/20 | loss 0.0051\n",
            "Epoch  5/20 | loss 0.0035\n",
            "Epoch  6/20 | loss 0.0025\n",
            "Epoch  7/20 | loss 0.0020\n",
            "Epoch  8/20 | loss 0.0016\n",
            "Epoch  9/20 | loss 0.0013\n",
            "Epoch 10/20 | loss 0.0011\n",
            "Epoch 11/20 | loss 0.0009\n",
            "Epoch 12/20 | loss 0.0008\n",
            "Epoch 13/20 | loss 0.0007\n",
            "Epoch 14/20 | loss 0.0006\n",
            "Epoch 15/20 | loss 0.0005\n",
            "Epoch 16/20 | loss 0.0005\n",
            "Epoch 17/20 | loss 0.0004\n",
            "Epoch 18/20 | loss 0.0004\n",
            "Epoch 19/20 | loss 0.0003\n",
            "Epoch 20/20 | loss 0.0003\n",
            "🔍 Evaluating…\n",
            "✅ Precision=0.000 Recall=0.000 F1=0.000\n"
          ]
        }
      ]
    }
  ]
}