{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "SE719lgjKPfo",
        "outputId": "71fca10f-4f33-4b8c-9f00-2ba0d0c37d1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m118.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m90.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m53.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m87.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing data.yaml at: /kaggle/input/fish-detection/data.yaml\n",
            "Creating new Ultralytics Settings v0.0.6 file âœ… \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8n.pt to 'yolov8n.pt'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6.25M/6.25M [00:00<00:00, 120MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics 8.3.134 ğŸš€ Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/kaggle/input/fish-detection/data.yaml, degrees=0.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=10, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.001, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=fish_baseline, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=runs/detect/fish_baseline, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
            "Downloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 755k/755k [00:00<00:00, 25.8MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding model.yaml nc=80 with nc=13\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
            " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
            " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 22        [15, 18, 21]  1    753847  ultralytics.nn.modules.head.Detect           [13, [64, 128, 256]]          \n",
            "Model summary: 129 layers, 3,013,383 parameters, 3,013,367 gradients, 8.2 GFLOPs\n",
            "\n",
            "Transferred 319/355 items from pretrained weights\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n.pt to 'yolo11n.pt'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5.35M/5.35M [00:00<00:00, 110MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 11.7Â±7.2 MB/s, size: 46.5 KB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /kaggle/input/fish-detection/train/labels... 6842 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6842/6842 [00:36<00:00, 187.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING âš ï¸ \u001b[34m\u001b[1mtrain: \u001b[0mCache directory /kaggle/input/fish-detection/train is not writeable, cache not saved.\n",
            "WARNING âš ï¸ Box and segment counts should be equal, but got len(segments) = 536, len(boxes) = 14154. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 14.7Â±7.9 MB/s, size: 37.0 KB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/input/fish-detection/valid/labels... 700 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 700/700 [00:03<00:00, 184.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING âš ï¸ \u001b[34m\u001b[1mval: \u001b[0mCache directory /kaggle/input/fish-detection/valid is not writeable, cache not saved.\n",
            "WARNING âš ï¸ Box and segment counts should be equal, but got len(segments) = 84, len(boxes) = 1197. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n",
            "Plotting labels to runs/detect/fish_baseline/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.001' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000588, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
            "Image sizes 640 train, 640 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1mruns/detect/fish_baseline\u001b[0m\n",
            "Starting training for 10 epochs...\n",
            "Closing dataloader mosaic\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       1/10      2.26G      1.343      3.115      1.614         13        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 428/428 [02:11<00:00,  3.25it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:09<00:00,  2.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        700       1197      0.656      0.547      0.639      0.406\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       2/10      2.66G      1.324      2.083      1.541         15        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 428/428 [02:04<00:00,  3.44it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:07<00:00,  2.89it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        700       1197      0.723      0.636      0.706      0.418\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       3/10      2.68G      1.272      1.669      1.495         25        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 428/428 [02:04<00:00,  3.44it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:07<00:00,  2.91it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        700       1197      0.689      0.674       0.73      0.466\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       4/10       2.7G      1.242      1.389       1.46         14        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 428/428 [02:04<00:00,  3.44it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:07<00:00,  2.95it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        700       1197      0.742      0.702      0.793      0.531\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       5/10      2.71G      1.192      1.188      1.413         19        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 428/428 [02:04<00:00,  3.45it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:07<00:00,  2.95it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        700       1197      0.752      0.792      0.822       0.55\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       6/10      2.73G      1.147      1.051      1.376         23        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 428/428 [02:03<00:00,  3.47it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:07<00:00,  2.97it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        700       1197      0.847      0.822      0.875      0.601\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       7/10      2.75G        1.1     0.9456      1.349         11        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 428/428 [02:06<00:00,  3.37it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:07<00:00,  2.99it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        700       1197      0.856       0.82      0.889      0.628\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       8/10      2.76G      1.061     0.8579      1.311         24        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 428/428 [02:04<00:00,  3.44it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:07<00:00,  2.96it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        700       1197      0.866       0.82      0.885      0.636\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       9/10      2.78G      1.014     0.7768      1.277         11        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 428/428 [02:06<00:00,  3.40it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:07<00:00,  2.98it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        700       1197      0.888      0.853      0.908       0.65\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      10/10       2.8G     0.9878     0.7319      1.254         34        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 428/428 [02:04<00:00,  3.44it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:07<00:00,  2.93it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        700       1197      0.872      0.874      0.913      0.667\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "10 epochs completed in 0.371 hours.\n",
            "Optimizer stripped from runs/detect/fish_baseline/weights/last.pt, 6.2MB\n",
            "Optimizer stripped from runs/detect/fish_baseline/weights/best.pt, 6.2MB\n",
            "\n",
            "Validating runs/detect/fish_baseline/weights/best.pt...\n",
            "Ultralytics 8.3.134 ğŸš€ Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
            "Model summary (fused): 72 layers, 3,008,183 parameters, 0 gradients, 8.1 GFLOPs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:10<00:00,  2.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        700       1197      0.871      0.875      0.912      0.667\n",
            "              BlueTang          7         23      0.872      0.913      0.902      0.812\n",
            "         ButterflyFish         10         47      0.846      0.894        0.9      0.483\n",
            "             ClownFish         28         41      0.877      0.872      0.941       0.64\n",
            "              GoldFish         62        134      0.845      0.937      0.934      0.658\n",
            "               Gourami        188        210      0.968          1      0.995       0.91\n",
            "            MorishIdol         23         31      0.936      0.942      0.986      0.676\n",
            "             PlatyFish         50        102      0.786      0.755      0.835      0.594\n",
            "     RibbonedSweetlips         22         67      0.798      0.836      0.876      0.655\n",
            "ThreeStripedDamselfish         79        108      0.962       0.95      0.989      0.797\n",
            "         YellowCichlid         22         95      0.832      0.484      0.648      0.362\n",
            "            YellowTang         45         80      0.785      0.925      0.949      0.604\n",
            "             ZebraFish        164        259      0.948      0.987      0.993      0.815\n",
            "Speed: 0.3ms preprocess, 2.4ms inference, 0.0ms loss, 3.6ms postprocess per image\n",
            "Results saved to \u001b[1mruns/detect/fish_baseline\u001b[0m\n",
            "\n",
            "ğŸ“Š Baseline metrics:\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'DetMetrics' object has no attribute 'metrics'. See valid attributes below.\n\n    Utility class for computing detection metrics such as precision, recall, and mean average precision (mAP).\n\n    Attributes:\n        save_dir (Path): A path to the directory where the output plots will be saved.\n        plot (bool): A flag that indicates whether to plot precision-recall curves for each class.\n        names (dict): A dictionary of class names.\n        box (Metric): An instance of the Metric class for storing detection results.\n        speed (dict): A dictionary for storing execution times of different parts of the detection process.\n        task (str): The task type, set to 'detect'.\n    ",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-9addc0a09177>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# 4) Ğ’Ñ‹Ğ²Ğ¾Ğ´Ğ¸Ğ¼ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nğŸ“Š Baseline metrics:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvals\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults_baseline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{metric}: {vals[-1]:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ultralytics/utils/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0;34m\"\"\"Custom attribute access error message with helpful information.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"'{name}' object has no attribute '{attr}'. See valid attributes below.\\n{self.__doc__}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'DetMetrics' object has no attribute 'metrics'. See valid attributes below.\n\n    Utility class for computing detection metrics such as precision, recall, and mean average precision (mAP).\n\n    Attributes:\n        save_dir (Path): A path to the directory where the output plots will be saved.\n        plot (bool): A flag that indicates whether to plot precision-recall curves for each class.\n        names (dict): A dictionary of class names.\n        box (Metric): An instance of the Metric class for storing detection results.\n        speed (dict): A dictionary for storing execution times of different parts of the detection process.\n        task (str): The task type, set to 'detect'.\n    "
          ]
        }
      ],
      "source": [
        "!pip install ultralytics -q\n",
        "\n",
        "import kagglehub, os\n",
        "dataset_str = kagglehub.dataset_download(\"zehraatlgan/fish-detection\")\n",
        "data_yaml = os.path.join(dataset_str, \"data.yaml\")\n",
        "print(\"Using data.yaml at:\", data_yaml)\n",
        "\n",
        "from ultralytics import YOLO\n",
        "\n",
        "# Ğ‘ĞµĞ¹Ğ·Ğ»Ğ°Ğ¹Ğ½\n",
        "model = YOLO('yolov8n.pt')\n",
        "results_baseline = model.train(\n",
        "    data=data_yaml,\n",
        "    epochs=10,\n",
        "    imgsz=640,\n",
        "    batch=16,\n",
        "    lr0=1e-3,\n",
        "    name='fish_baseline'\n",
        ")\n",
        "\n",
        "print(\"\\n Baseline metrics:\")\n",
        "for metric, vals in results_baseline.metrics.items():\n",
        "    print(f\"{metric}: {vals[-1]:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "metrics = results_baseline.results_dict\n",
        "\n",
        "for name, value in metrics.items():\n",
        "    print(f\"{name}: {value:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NfUnc422OwIF",
        "outputId": "7d829161-9d6d-40f6-f383-ddca4315aaa4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "metrics/precision(B): 0.8714\n",
            "metrics/recall(B): 0.8746\n",
            "metrics/mAP50(B): 0.9124\n",
            "metrics/mAP50-95(B): 0.6671\n",
            "fitness: 0.6916\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EqE9hVJgRNy8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ğ”Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ mAP Ğ¼Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ 12-ÑĞ¿Ğ¾Ñ…Ğ¾Ğ²ÑƒÑ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ YOLOv8n Ñ Ğ°Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ°ÑƒĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ (RandAugment, Copy-Paste, MixUp Ğ¸ Mosaic), Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ¼ AdamW Ğ¸ cosine-LR scheduler, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñ‹ Ğ¸ ÑĞ³Ğ»Ğ°Ğ´Ğ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TCfVaAPbR4QJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = YOLO('yolov8n.pt')\n",
        "\n",
        "# 3) Ğ¢Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²ĞºĞ°: 12 ÑĞ¿Ğ¾Ñ…, Ğ°Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ğµ Ğ°ÑƒĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸, AdamW + cosine LR\n",
        "results = model.train(\n",
        "    data=data_yaml,\n",
        "    epochs=12,\n",
        "    imgsz=640,\n",
        "    batch=16,\n",
        "    lr0=5e-4,\n",
        "    optimizer='AdamW',\n",
        "    cos_lr=True,\n",
        "    augment=True,\n",
        "    auto_augment='v0',\n",
        "    copy_paste=0.2,\n",
        "    mixup=0.2,\n",
        "    mosaic=0.5,\n",
        "    fliplr=0.5,\n",
        "    flipud=0.3,\n",
        "    hsv_h=0.02, hsv_s=0.8, hsv_v=0.5,\n",
        "    name='fish_augmented_12ep'\n",
        ")\n",
        "\n",
        "metrics = results.results_dict\n",
        "print(\"\\n Metrics after 12 epochs on augmented data:\")\n",
        "print(f\"Precision:    {metrics['precision']:.4f}\")\n",
        "print(f\"Recall:       {metrics['recall']:.4f}\")\n",
        "print(f\"mAP@50:       {metrics['map50']:.4f}\")\n",
        "print(f\"mAP@50-95:    {metrics['map50_95']:.4f}\")\n",
        "print(f\"Fitness:      {metrics.get('fitness', float('nan')):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "W13BNqcBR5kT",
        "outputId": "bcb927ec-c890-48b5-bdcc-dcf810bcd90b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics 8.3.134 ğŸš€ Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=True, auto_augment=v0, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.2, copy_paste_mode=flip, cos_lr=True, cutmix=0.0, data=/kaggle/input/fish-detection/data.yaml, degrees=0.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=12, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.3, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.02, hsv_s=0.8, hsv_v=0.5, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.0005, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.2, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=0.5, multi_scale=False, name=fish_augmented_12ep, nbs=64, nms=False, opset=None, optimize=False, optimizer=AdamW, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=runs/detect/fish_augmented_12ep, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
            "Overriding model.yaml nc=80 with nc=13\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
            " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
            " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 22        [15, 18, 21]  1    753847  ultralytics.nn.modules.head.Detect           [13, [64, 128, 256]]          \n",
            "Model summary: 129 layers, 3,013,383 parameters, 3,013,367 gradients, 8.2 GFLOPs\n",
            "\n",
            "Transferred 319/355 items from pretrained weights\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access âœ… (ping: 0.1Â±0.1 ms, read: 78.1Â±17.4 MB/s, size: 46.5 KB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /kaggle/input/fish-detection/train/labels... 6842 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6842/6842 [00:11<00:00, 570.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING âš ï¸ \u001b[34m\u001b[1mtrain: \u001b[0mCache directory /kaggle/input/fish-detection/train is not writeable, cache not saved.\n",
            "WARNING âš ï¸ Box and segment counts should be equal, but got len(segments) = 536, len(boxes) = 14154. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.2Â±0.2 ms, read: 101.0Â±39.2 MB/s, size: 37.0 KB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/input/fish-detection/valid/labels... 700 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 700/700 [00:01<00:00, 498.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING âš ï¸ \u001b[34m\u001b[1mval: \u001b[0mCache directory /kaggle/input/fish-detection/valid is not writeable, cache not saved.\n",
            "WARNING âš ï¸ Box and segment counts should be equal, but got len(segments) = 84, len(boxes) = 1197. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Plotting labels to runs/detect/fish_augmented_12ep/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.0005, momentum=0.937) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
            "Image sizes 640 train, 640 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1mruns/detect/fish_augmented_12ep\u001b[0m\n",
            "Starting training for 12 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       1/12      2.71G      1.559      2.876      1.709         27        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 428/428 [02:36<00:00,  2.74it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:07<00:00,  2.91it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        700       1197      0.513       0.51      0.494       0.31\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       2/12      2.98G      1.512      2.075      1.653         46        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 428/428 [02:28<00:00,  2.88it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:06<00:00,  3.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        700       1197       0.55       0.57      0.589      0.366\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Closing dataloader mosaic\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       3/12      2.98G      1.335      1.512      1.585         28        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 428/428 [02:07<00:00,  3.35it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:06<00:00,  3.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        700       1197      0.627      0.715      0.724      0.446\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       4/12      2.98G      1.274      1.301      1.509         14        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 428/428 [02:08<00:00,  3.33it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:06<00:00,  3.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        700       1197       0.68       0.73       0.74       0.48\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       5/12         3G      1.225      1.166       1.47         19        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 428/428 [02:07<00:00,  3.37it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:07<00:00,  3.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        700       1197      0.762      0.724      0.784      0.522\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       6/12      3.01G      1.197      1.066      1.439         23        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 428/428 [02:07<00:00,  3.36it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:07<00:00,  3.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        700       1197      0.735      0.785      0.817       0.56\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       7/12      3.03G      1.147     0.9773      1.412         11        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 428/428 [02:04<00:00,  3.45it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:07<00:00,  2.96it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        700       1197      0.826        0.8      0.857      0.601\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       8/12      3.05G      1.109     0.9045      1.375         24        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 428/428 [02:06<00:00,  3.37it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:07<00:00,  3.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        700       1197      0.841      0.826      0.885       0.62\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       9/12      3.06G      1.073     0.8271      1.339         11        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 428/428 [02:07<00:00,  3.36it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:07<00:00,  3.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        700       1197      0.851      0.841       0.88      0.639\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      10/12      3.08G      1.038     0.7729      1.322         34        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 428/428 [02:04<00:00,  3.43it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:07<00:00,  3.12it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        700       1197      0.845      0.878      0.902      0.653\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      11/12       3.1G      1.017     0.7456      1.298         21        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 428/428 [02:04<00:00,  3.43it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:08<00:00,  2.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        700       1197      0.845      0.889      0.903      0.664\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      12/12      3.12G      1.008     0.7289      1.296         24        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 428/428 [02:05<00:00,  3.42it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:07<00:00,  3.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        700       1197      0.839      0.886        0.9      0.665\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "12 epochs completed in 0.461 hours.\n",
            "Optimizer stripped from runs/detect/fish_augmented_12ep/weights/last.pt, 6.2MB\n",
            "Optimizer stripped from runs/detect/fish_augmented_12ep/weights/best.pt, 6.2MB\n",
            "\n",
            "Validating runs/detect/fish_augmented_12ep/weights/best.pt...\n",
            "Ultralytics 8.3.134 ğŸš€ Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
            "Model summary (fused): 72 layers, 3,008,183 parameters, 0 gradients, 8.1 GFLOPs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:10<00:00,  2.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        700       1197      0.839      0.873      0.909      0.676\n",
            "              BlueTang          7         23      0.869      0.864      0.922      0.831\n",
            "         ButterflyFish         10         47      0.736      0.809       0.81      0.452\n",
            "             ClownFish         28         41      0.845      0.976      0.946      0.658\n",
            "              GoldFish         62        134      0.795      0.924      0.939      0.694\n",
            "               Gourami        188        210      0.929          1      0.995      0.915\n",
            "            MorishIdol         23         31      0.876      0.903      0.967      0.767\n",
            "             PlatyFish         50        102      0.768      0.784      0.864      0.605\n",
            "     RibbonedSweetlips         22         67      0.758       0.91      0.899      0.646\n",
            "ThreeStripedDamselfish         79        108      0.895       0.95      0.979      0.771\n",
            "         YellowCichlid         22         95      0.895      0.449      0.684      0.368\n",
            "            YellowTang         45         80       0.75      0.912      0.908      0.599\n",
            "             ZebraFish        164        259      0.956      0.996      0.995      0.808\n",
            "Speed: 0.3ms preprocess, 7.6ms inference, 0.0ms loss, 2.2ms postprocess per image\n",
            "Results saved to \u001b[1mruns/detect/fish_augmented_12ep\u001b[0m\n",
            "\n",
            "ğŸ“Š Metrics after 12 epochs on augmented data:\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'precision'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-9cfe85f255b1>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nğŸ“Š Metrics after 12 epochs on augmented data:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Precision:    {metrics['precision']:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Recall:       {metrics['recall']:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"mAP@50:       {metrics['map50']:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'precision'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "metrics = results.results_dict\n",
        "\n",
        "for name, value in metrics.items():\n",
        "    print(f\"{name}: {value:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZ4XqQUqYrDC",
        "outputId": "525a7032-e22f-4ecb-eaea-371c9bb20abc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "metrics/precision(B): 0.8393\n",
            "metrics/recall(B): 0.8731\n",
            "metrics/mAP50(B): 0.9091\n",
            "metrics/mAP50-95(B): 0.6762\n",
            "fitness: 0.6995\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ğ¡ÑƒĞ´Ñ Ğ¿Ğ¾ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ°Ğ¼, Ğ°Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ğµ Ğ°ÑƒĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¼ĞµÑÑ‚Ğµ Ñ AdamW + cosine Ğ´Ğ°Ğ»Ğ¸ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ¿Ñ€Ğ¸Ñ€Ğ¾ÑÑ‚ mAP50-95 (0.6762 vs 0.6671), Ğ½Ğ¾ Ğ·Ğ°Ğ¼ĞµÑ‚Ğ½Ğ¾ ÑĞ½Ğ¸Ğ·Ğ¸Ğ»Ğ¸ precision (0.8393 vs 0.8714) Ğ¸ ÑĞ»ĞµĞ³ĞºĞ° ÑƒĞ¿Ğ°Ğ»Ğ¸ mAP50 (0.9091 vs 0.9124) â€” Ñ‚Ğ¾ ĞµÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ»ÑƒÑ‡ÑˆĞµ Ğ½Ğ°ÑƒÑ‡Ğ¸Ğ»Ğ°ÑÑŒ Â«Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Â» Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ñ‹ Ğ¿Ğ¾ IoU, Ğ½Ğ¾ ÑÑ‚Ğ°Ğ»Ğ° Ñ‡Ğ°Ñ‰Ğµ Ğ´ĞµĞ»Ğ°Ñ‚ÑŒ Ğ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ ÑÑ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ."
      ],
      "metadata": {
        "id": "2Y5crgeWY4-i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ÑĞ¾Ğ±ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ°Ñ Ğ¸Ğ¼Ğ¿Ğ»ĞµĞ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ"
      ],
      "metadata": {
        "id": "YlYT97CCZMAg"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9-g2hGo0ZNp-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import yaml\n",
        "import glob\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as T\n",
        "import kagglehub\n",
        "\n",
        "# 1. Download dataset and get path\n",
        "dataset_str = kagglehub.dataset_download(\"zehraatlgan/fish-detection\")\n",
        "data_yaml = os.path.join(dataset_str, \"data.yaml\")\n",
        "print(\"Using data.yaml at:\", data_yaml)\n",
        "\n",
        "# 2. Load dataset configuration\n",
        "with open(data_yaml, 'r') as f:\n",
        "    data_cfg = yaml.safe_load(f)\n",
        "\n",
        "# 3. Resolve image directories (handle various relative paths)\n",
        "base_dir = os.path.dirname(data_yaml)\n",
        "train_candidates = [\n",
        "    os.path.normpath(os.path.join(base_dir, data_cfg['train'])),\n",
        "    os.path.normpath(os.path.join(dataset_str, data_cfg['train'])),\n",
        "    os.path.join(dataset_str, 'train/images'),\n",
        "    os.path.join(dataset_str, 'train'),\n",
        "]\n",
        "val_candidates = [\n",
        "    os.path.normpath(os.path.join(base_dir, data_cfg['val'])),\n",
        "    os.path.normpath(os.path.join(dataset_str, data_cfg['val'])),\n",
        "    os.path.join(dataset_str, 'valid/images'),\n",
        "    os.path.join(dataset_str, 'valid'),\n",
        "    os.path.join(dataset_str, 'val/images'),\n",
        "    os.path.join(dataset_str, 'val'),\n",
        "]\n",
        "\n",
        "# Select first existing directory\n",
        "train_img_dir = next((d for d in train_candidates if os.path.isdir(d)), None)\n",
        "val_img_dir   = next((d for d in val_candidates   if os.path.isdir(d)), None)\n",
        "if train_img_dir is None:\n",
        "    raise FileNotFoundError(f\"No valid train directory found among: {train_candidates}\")\n",
        "if val_img_dir is None:\n",
        "    raise FileNotFoundError(f\"No valid val directory found among: {val_candidates}\")\n",
        "\n",
        "print(f\"Using train images from: {train_img_dir}\")\n",
        "print(f\"Using val images from:   {val_img_dir}\")\n",
        "\n",
        "# Class names\n",
        "class_names = data_cfg.get('names', {})\n",
        "num_classes = len(class_names)\n",
        "\n",
        "# 4. Prepare small subset\n",
        "train_imgs = sorted(glob.glob(os.path.join(train_img_dir, '*.jpg')))[:100]\n",
        "val_imgs   = sorted(glob.glob(os.path.join(val_img_dir,   '*.jpg')))[:50]\n",
        "if not train_imgs:\n",
        "    raise RuntimeError(f\"No JPG images found in train dir: {train_img_dir}\")\n",
        "if not val_imgs:\n",
        "    raise RuntimeError(f\"No JPG images found in val dir: {val_img_dir}\")\n",
        "\n",
        "# 5. Dataset class\n",
        "class FishDataset(Dataset):\n",
        "    def __init__(self, img_paths, img_size=640, grid_size=20):\n",
        "        self.img_paths = img_paths\n",
        "        self.img_size  = img_size\n",
        "        self.grid_size = grid_size\n",
        "        self.transform = T.Compose([\n",
        "            T.Resize((img_size, img_size)),\n",
        "            T.ToTensor(),\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.img_paths[idx]\n",
        "        txt_path = img_path.replace('.jpg', '.txt')\n",
        "\n",
        "        img = Image.open(img_path).convert('RGB')\n",
        "        img = self.transform(img)\n",
        "\n",
        "        S = self.grid_size\n",
        "        target = torch.zeros((5, S, S))\n",
        "\n",
        "        if os.path.isfile(txt_path):\n",
        "            for line in open(txt_path):\n",
        "                cls, x, y, w, h = map(float, line.split())\n",
        "                gx, gy = x * S, y * S\n",
        "                gi, gj = int(gx), int(gy)\n",
        "                tx, ty = gx - gi, gy - gj\n",
        "                tw, th = w * S, h * S\n",
        "                target[0, gj, gi] = tx\n",
        "                target[1, gj, gi] = ty\n",
        "                target[2, gj, gi] = tw\n",
        "                target[3, gj, gi] = th\n",
        "                target[4, gj, gi] = 1.0\n",
        "\n",
        "        return img, target\n",
        "\n",
        "def collate_fn(batch):\n",
        "    imgs, targets = zip(*batch)\n",
        "    return torch.stack(imgs), torch.stack(targets)\n",
        "\n",
        "# 6. Model definition\n",
        "class SimpleYOLO(nn.Module):\n",
        "    def __init__(self, grid_size=20):\n",
        "        super().__init__()\n",
        "        self.backbone = nn.Sequential(\n",
        "            nn.Conv2d(3, 16, 3, 1, 1), nn.ReLU(), nn.MaxPool2d(2),\n",
        "            nn.Conv2d(16, 32, 3, 1, 1), nn.ReLU(), nn.MaxPool2d(2),\n",
        "            nn.Conv2d(32, 64, 3, 1, 1), nn.ReLU(), nn.MaxPool2d(2),\n",
        "            nn.Conv2d(64, 128,3, 1, 1), nn.ReLU(), nn.MaxPool2d(2),\n",
        "            nn.Conv2d(128,256,3, 1, 1), nn.ReLU(), nn.MaxPool2d(2)\n",
        "        )\n",
        "        self.pred = nn.Conv2d(256, 5, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.pred(self.backbone(x))\n",
        "\n",
        "# 7. Setup data loaders\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "train_ds = FishDataset(train_imgs)\n",
        "train_dl = DataLoader(train_ds, batch_size=8, shuffle=True, collate_fn=collate_fn)\n",
        "val_ds   = FishDataset(val_imgs)\n",
        "val_dl   = DataLoader(val_ds,   batch_size=8, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "# 8. Training loop\n",
        "def train_one_epoch(model, dataloader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    for imgs, targets in dataloader:\n",
        "        imgs, targets = imgs.to(device), targets.to(device)\n",
        "        preds = model(imgs)\n",
        "        loss  = criterion(preds, targets)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward() # ĞµĞ±Ğ°Ğ½ÑƒÑ‚Ñ‹Ğµ Ğ±Ğ»ÑÑ‚ÑŒ\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "model     = SimpleYOLO().to(device)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=5e-4)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "epochs = 5\n",
        "for epoch in range(1, epochs+1):\n",
        "    loss = train_one_epoch(model, train_dl, optimizer, criterion, device)\n",
        "    print(f\"Epoch {epoch}/{epochs} - Loss: {loss:.4f}\")\n",
        "\n",
        "# 9. Save\n",
        "torch.save(model.state_dict(), 'simple_yolo_fish.pth')\n",
        "print(\"Training complete. Model saved to simple_yolo_fish.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "drrLtUvLZjnJ",
        "outputId": "4664d35d-2340-4da9-949a-078b70ddecec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using data.yaml at: /kaggle/input/fish-detection/data.yaml\n",
            "Using train images from: /kaggle/input/fish-detection/train/images\n",
            "Using val images from:   /kaggle/input/fish-detection/valid/images\n",
            "Epoch 1/5 - Loss: 0.0002\n",
            "Epoch 2/5 - Loss: 0.0000\n",
            "Epoch 3/5 - Loss: 0.0000\n",
            "Epoch 4/5 - Loss: 0.0000\n",
            "Epoch 5/5 - Loss: 0.0000\n",
            "Training complete. Model saved to simple_yolo_fish.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = SimpleYOLO().to(device)\n",
        "model.load_state_dict(torch.load('simple_yolo_fish.pth'))\n",
        "model.eval()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZfyNqqzMdeP4",
        "outputId": "8e6be5e6-aad5-4805-9eae-b4f2b566f5ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SimpleYOLO(\n",
              "  (backbone): Sequential(\n",
              "    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU()\n",
              "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (3): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (4): ReLU()\n",
              "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (6): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (7): ReLU()\n",
              "    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (9): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (10): ReLU()\n",
              "    (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (12): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (13): ReLU()\n",
              "    (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (pred): Conv2d(256, 5, kernel_size=(1, 1), stride=(1, 1))\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision.ops import box_iou\n",
        "\n",
        "def decode_preds(pred, S=20, img_size=640, conf_thr=0.5):\n",
        "    \"\"\"\n",
        "    ĞĞ°Ñ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ² ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼ Ğ±Ğ°Ñ‚Ñ‡Ğµ Ğ²ÑĞµ ÑÑ‡ĞµĞ¹ĞºĞ¸ Ñ conf>conf_thr\n",
        "    Ğ¸ Ğ¿Ñ€ĞµĞ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ğ¸Ñ… Ğ² Ğ±Ğ¾ĞºÑÑ‹ (x1,y1,x2,y2) Ğ² Ğ¿Ğ¸ĞºÑĞµĞ»ÑÑ….\n",
        "    \"\"\"\n",
        "    B = pred.size(0)\n",
        "    cell = img_size / S\n",
        "    all_preds = []\n",
        "    for b in range(B):\n",
        "        p = pred[b]  # [5,S,S]\n",
        "        objs = p[4]\n",
        "        idxs = (objs > conf_thr).nonzero(as_tuple=False)\n",
        "        boxes, scores = [], []\n",
        "        for gj, gi in idxs:\n",
        "            tx, ty, tw, th = p[0,gj,gi], p[1,gj,gi], p[2,gj,gi], p[3,gj,gi]\n",
        "            cx = (gi + tx) * cell\n",
        "            cy = (gj + ty) * cell\n",
        "            w  = tw * cell\n",
        "            h  = th * cell\n",
        "            x1, y1 = cx - w/2, cy - h/2\n",
        "            x2, y2 = cx + w/2, cy + h/2\n",
        "            boxes.append([x1,y1,x2,y2])\n",
        "            scores.append(objs[gj,gi].item())\n",
        "        if boxes:\n",
        "            all_preds.append({\n",
        "                'boxes': torch.tensor(boxes),\n",
        "                'scores': torch.tensor(scores)\n",
        "            })\n",
        "        else:\n",
        "            all_preds.append({'boxes': torch.zeros((0,4)), 'scores': torch.zeros((0,))})\n",
        "    return all_preds\n",
        "\n",
        "def get_gt_boxes(target, S=20, img_size=640):\n",
        "    \"\"\"\n",
        "    Ğ˜Ğ· Ñ‚Ğ°Ñ€Ğ³ĞµÑ‚Ğ° [5,S,S] Ğ²Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ñ‚ĞµĞ½Ğ·Ğ¾Ñ€ [N,4] Ğ¸ÑÑ‚Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ±Ğ¾ĞºÑĞ¾Ğ².\n",
        "    \"\"\"\n",
        "    cell = img_size / S\n",
        "    obj = target[4]\n",
        "    idxs = (obj > 0.5).nonzero(as_tuple=False)\n",
        "    boxes = []\n",
        "    for gj, gi in idxs:\n",
        "        tx, ty, tw, th = target[0,gj,gi], target[1,gj,gi], target[2,gj,gi], target[3,gj,gi]\n",
        "        cx = (gi + tx) * cell\n",
        "        cy = (gj + ty) * cell\n",
        "        w  = tw * cell\n",
        "        h  = th * cell\n",
        "        x1, y1 = cx - w/2, cy - h/2\n",
        "        x2, y2 = cx + w/2, cy + h/2\n",
        "        boxes.append([x1,y1,x2,y2])\n",
        "    return torch.tensor(boxes)\n"
      ],
      "metadata": {
        "id": "v6d8CtK0djxj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_tp = all_fp = all_fn = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for imgs, targets in val_dl:\n",
        "        imgs = imgs.to(device)\n",
        "        preds = model(imgs).cpu()\n",
        "        batch_preds = decode_preds(preds, S=20, img_size=640, conf_thr=0.5)\n",
        "\n",
        "        for i in range(len(batch_preds)):\n",
        "            pred = batch_preds[i]\n",
        "            gt   = get_gt_boxes(targets[i], S=20, img_size=640)\n",
        "\n",
        "            if len(pred['boxes']) == 0:\n",
        "                all_fn += len(gt); continue\n",
        "            if len(gt) == 0:\n",
        "                all_fp += len(pred['boxes']); continue\n",
        "\n",
        "            ious = box_iou(pred['boxes'], gt)\n",
        "            # Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ¸ĞºÑ‚Ğ° Ğ±ĞµÑ€Ñ‘Ğ¼ Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ IoU\n",
        "            max_iou, _ = ious.max(dim=1)  # [P]\n",
        "            tp = (max_iou >= 0.5).sum().item()\n",
        "            fp = (max_iou < 0.5).sum().item()\n",
        "            fn = len(gt) - tp\n",
        "\n",
        "            all_tp += tp\n",
        "            all_fp += fp\n",
        "            all_fn += fn\n",
        "\n",
        "precision = all_tp / (all_tp + all_fp + 1e-8)\n",
        "recall    = all_tp / (all_tp + all_fn + 1e-8)\n",
        "f1        = 2 * precision * recall / (precision + recall + 1e-8)\n",
        "\n",
        "print(f\"Val â€” Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m3sengUCdnHu",
        "outputId": "f755a267-f760-403a-fc4c-ad1c1d148a8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val â€” Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import yaml\n",
        "import glob\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as T\n",
        "import kagglehub\n",
        "from torchvision.ops import box_iou\n",
        "\n",
        "# =========================\n",
        "# 1. DATA LOADING\n",
        "# =========================\n",
        "print(\"Downloading dataset (cached after first run)â€¦\")\n",
        "root = kagglehub.dataset_download(\"zehraatlgan/fish-detection\")\n",
        "ctx_yaml = os.path.join(root, \"data.yaml\")\n",
        "print(\"Using data.yaml:\", ctx_yaml)\n",
        "\n",
        "with open(ctx_yaml) as f:\n",
        "    cfg = yaml.safe_load(f)\n",
        "train_dir = os.path.normpath(os.path.join(root, cfg[\"train\"].lstrip(\"../\")))\n",
        "val_dir   = os.path.normpath(os.path.join(root, cfg[\"val\"].lstrip(\"../\")))\n",
        "print(\"Train dir:\", train_dir)\n",
        "print(\"Val dir  :\", val_dir)\n",
        "\n",
        "train_imgs = sorted(glob.glob(os.path.join(train_dir, \"*.jpg\")))[:128]\n",
        "val_imgs   = sorted(glob.glob(os.path.join(val_dir,   \"*.jpg\")))[:64]\n",
        "assert train_imgs and val_imgs, \"Images not found!\"\n",
        "IMG_SIZE = 416  # reduce resolution to cut memory\n",
        "\n",
        "# =========================\n",
        "# 2. DATASET\n",
        "# =========================\n",
        "class FishDataset(Dataset):\n",
        "    def __init__(self, paths, S=13, augment=False):\n",
        "        self.paths = paths; self.S=S\n",
        "        base = [T.Resize((IMG_SIZE, IMG_SIZE)), T.ToTensor()]\n",
        "        aug  = [T.RandomHorizontalFlip()] if augment else []\n",
        "        self.tf = T.Compose(aug+base)\n",
        "    def __len__(self): return len(self.paths)\n",
        "    def __getitem__(self, idx):\n",
        "        p = self.paths[idx]\n",
        "        img = self.tf(Image.open(p).convert('RGB'))\n",
        "        t = torch.zeros((5,self.S,self.S))\n",
        "        txt=p.replace('.jpg','.txt')\n",
        "        if os.path.exists(txt):\n",
        "            for l in open(txt):\n",
        "                _,x,y,w,h=map(float,l.split())\n",
        "                gx,gy=x*self.S,y*self.S; gi,gj=int(gx),int(gy)\n",
        "                t[0,gj,gi]=gx-gi; t[1,gj,gi]=gy-gj; t[2,gj,gi]=w*self.S; t[3,gj,gi]=h*self.S; t[4,gj,gi]=1.\n",
        "        return img,t\n",
        "\n",
        "def collate(b):\n",
        "    a,b=zip(*b); return torch.stack(a),torch.stack(b)\n",
        "\n",
        "# =========================\n",
        "# 3. MODEL (lighter)\n",
        "# =========================\n",
        "class TinyYOLO(nn.Module):\n",
        "    def __init__(self,S=13):\n",
        "        super().__init__()\n",
        "        def C(i,o): return nn.Sequential(nn.Conv2d(i,o,3,1,1), nn.BatchNorm2d(o), nn.LeakyReLU(0.1))\n",
        "        self.back = nn.Sequential(\n",
        "            C(3,16), nn.MaxPool2d(2),\n",
        "            C(16,32), nn.MaxPool2d(2),\n",
        "            C(32,64), nn.MaxPool2d(2),\n",
        "            C(64,128),nn.MaxPool2d(2),\n",
        "            C(128,256),nn.MaxPool2d(2)\n",
        "        )\n",
        "        self.pred = nn.Conv2d(256,5,1)\n",
        "        self.S=S\n",
        "    def forward(self,x): return self.pred(self.back(x))\n",
        "\n",
        "# =========================\n",
        "# 4. LOSS\n",
        "# =========================\n",
        "L1=nn.SmoothL1Loss()\n",
        "\n",
        "def loss_fn(p,t,Î»c=5,Î»o=1,Î»n=0.5,pos_w=15.):\n",
        "    obj=t[:,4:5]>0; bce=nn.BCEWithLogitsLoss(pos_weight=torch.tensor([pos_w],device=p.device))\n",
        "    lo=bce(p[:,4:5][obj],torch.ones_like(p[:,4:5][obj])) if obj.any() else torch.tensor(0.,device=p.device)\n",
        "    lno=bce(p[:,4:5][~obj],torch.zeros_like(p[:,4:5][~obj]))\n",
        "    lb=L1(p[:,:4][obj.expand_as(p[:,:4])],t[:,:4][obj.expand_as(t[:,:4])]) if obj.any() else torch.tensor(0.,device=p.device)\n",
        "    return Î»c*lb+Î»o*lo+Î»n*lno\n",
        "\n",
        "# =========================\n",
        "# 5. TRAIN\n",
        "# =========================\n",
        "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model=TinyYOLO().to(device)\n",
        "opt=optim.Adam(model.parameters(),lr=1e-3)\n",
        "train_dl=DataLoader(FishDataset(train_imgs,augment=True),batch_size=4,shuffle=True,collate_fn=collate)\n",
        "val_dl=DataLoader(FishDataset(val_imgs),batch_size=4,collate_fn=collate)\n",
        "\n",
        "epochs=10\n",
        "for ep in range(1,epochs+1):\n",
        "    model.train();run=0\n",
        "    for x,y in train_dl:\n",
        "        x,y=x.to(device),y.to(device)\n",
        "        out=model(x); loss=loss_fn(out,y)\n",
        "        opt.zero_grad(); loss.backward(); opt.step(); run+=loss.item()\n",
        "    torch.cuda.empty_cache()\n",
        "    print(f\"E{ep}/{epochs} loss {run/len(train_dl):.4f}\")\n",
        "\n",
        "# =========================\n",
        "# 6. VERY ROUGH VAL (counts any pred as TP)\n",
        "# =========================\n",
        "model.eval(); tp=len(val_imgs); fp=fn=0\n",
        "print(f\"Dummy Precision=1.0 Recall=1.0 (placeholder)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YBJbJiB-iJcg",
        "outputId": "61570b0b-ea8d-4dd3-ba23-eda0e62f9f68"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading dataset (cached after first run)â€¦\n",
            "Resuming download from 128974848 bytes (229544954 bytes left)...\n",
            "Resuming download from https://www.kaggle.com/api/v1/datasets/download/zehraatlgan/fish-detection?dataset_version_number=1 (128974848/358519802) bytes left.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 342M/342M [00:10<00:00, 21.1MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using data.yaml: /root/.cache/kagglehub/datasets/zehraatlgan/fish-detection/versions/1/data.yaml\n",
            "Train dir: /root/.cache/kagglehub/datasets/zehraatlgan/fish-detection/versions/1/train/images\n",
            "Val dir  : /root/.cache/kagglehub/datasets/zehraatlgan/fish-detection/versions/1/valid/images\n",
            "E1/10 loss 0.0837\n",
            "E2/10 loss 0.0039\n",
            "E3/10 loss 0.0018\n",
            "E4/10 loss 0.0012\n",
            "E5/10 loss 0.0010\n",
            "E6/10 loss 0.0008\n",
            "E7/10 loss 0.0006\n",
            "E8/10 loss 0.0005\n",
            "E9/10 loss 0.0004\n",
            "E10/10 loss 0.0004\n",
            "Dummy Precision=1.0 Recall=1.0 (placeholder)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "j6AdqNTqjbq9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "js9_ZRhajeBk"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "as7yXh5kjbCT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, glob, yaml, torch, kagglehub\n",
        "import torch.nn as nn, torch.optim as optim\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as T\n",
        "from torchvision.ops import box_iou\n",
        "\n",
        "# =============== 1. DATA LOADING ===============\n",
        "print(\"â¬‡ï¸  Downloading dataset (ĞºĞµÑˆĞ¸Ñ€ÑƒĞµÑ‚ÑÑ)â€¦\")\n",
        "root = kagglehub.dataset_download(\"zehraatlgan/fish-detection\")\n",
        "yaml_path = os.path.join(root, \"data.yaml\")\n",
        "print(\"Using data.yaml:\", yaml_path)\n",
        "\n",
        "with open(yaml_path) as f:\n",
        "    cfg = yaml.safe_load(f)\n",
        "train_dir = os.path.normpath(os.path.join(root, cfg[\"train\"].lstrip(\"../\")))\n",
        "val_dir   = os.path.normpath(os.path.join(root, cfg[\"val\"].lstrip(\"../\")))\n",
        "print(\"Train dir:\", train_dir)\n",
        "print(\"Val dir  :\", val_dir)\n",
        "\n",
        "train_imgs = sorted(glob.glob(os.path.join(train_dir, \"*.jpg\")))[:256]\n",
        "val_imgs   = sorted(glob.glob(os.path.join(val_dir,   \"*.jpg\")))[:64]\n",
        "assert train_imgs and val_imgs, \"Images not found!\"\n",
        "\n",
        "IMG_SIZE = 416        # â†“ Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ 320, ĞµÑĞ»Ğ¸ Ğ¼Ğ°Ğ»Ğ¾ GPU-Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸\n",
        "GRID_S   = 13         # 416 / 32 â‰ˆ 13\n",
        "\n",
        "# =============== 2. DATASET ===============\n",
        "class FishDataset(Dataset):\n",
        "    def __init__(self, paths, S=GRID_S, augment=False):\n",
        "        self.paths, self.S = paths, S\n",
        "        base = [T.Resize((IMG_SIZE, IMG_SIZE)), T.ToTensor()]\n",
        "        aug  = [T.ColorJitter(0.2,0.2,0.2,0.1),\n",
        "                T.RandomHorizontalFlip()] if augment else []\n",
        "        self.tf = T.Compose(aug + base)\n",
        "    def __len__(self): return len(self.paths)\n",
        "    def __getitem__(self, i):\n",
        "        p = self.paths[i]\n",
        "        img = self.tf(Image.open(p).convert(\"RGB\"))\n",
        "        t = torch.zeros((5, self.S, self.S))\n",
        "        lab = p.replace(\".jpg\", \".txt\")\n",
        "        if os.path.exists(lab):\n",
        "            for line in open(lab):\n",
        "                _,x,y,w,h = map(float, line.split())\n",
        "                gx, gy = x*self.S, y*self.S\n",
        "                gi, gj = int(gx), int(gy)\n",
        "                if gi>=self.S or gj>=self.S: continue\n",
        "                t[0,gj,gi] = gx-gi;  t[1,gj,gi] = gy-gj\n",
        "                t[2,gj,gi] = w*self.S; t[3,gj,gi] = h*self.S\n",
        "                t[4,gj,gi] = 1.\n",
        "        return img, t\n",
        "\n",
        "def collate(b): imgs,tg = zip(*b); return torch.stack(imgs), torch.stack(tg)\n",
        "\n",
        "# =============== 3. MODEL ===============\n",
        "class CBL(nn.Sequential):\n",
        "    def __init__(self, c_in, c_out, k=3, s=1):\n",
        "        super().__init__(\n",
        "            nn.Conv2d(c_in, c_out, k, s, k//2, bias=False),\n",
        "            nn.BatchNorm2d(c_out),\n",
        "            nn.LeakyReLU(0.1, inplace=True)\n",
        "        )\n",
        "\n",
        "class TinyYOLO(nn.Module):\n",
        "    def __init__(self, S=GRID_S):\n",
        "        super().__init__()\n",
        "        self.back = nn.Sequential(\n",
        "            CBL(3,16),  nn.MaxPool2d(2),\n",
        "            CBL(16,32), nn.MaxPool2d(2),\n",
        "            CBL(32,64), nn.MaxPool2d(2),\n",
        "            CBL(64,128),nn.MaxPool2d(2),\n",
        "            CBL(128,256),nn.MaxPool2d(2),\n",
        "            CBL(256,256)\n",
        "        )\n",
        "        self.pred = nn.Conv2d(256, 5, 1)   # tx,ty,tw,th,conf\n",
        "    def forward(self,x): return self.pred(self.back(x))\n",
        "\n",
        "# =============== 4. LOSS ===============\n",
        "smooth_l1 = nn.SmoothL1Loss(reduction='mean')\n",
        "def yolo_loss(p, t, Î»c=5, Î»o=1, Î»n=0.5, pos_w=15.):\n",
        "    obj = t[:,4:5] > 0\n",
        "    bce = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([pos_w], device=p.device))\n",
        "    lo  = bce(p[:,4:5][obj], torch.ones_like(p[:,4:5][obj])) if obj.any() else torch.tensor(0., device=p.device)\n",
        "    lno = bce(p[:,4:5][~obj], torch.zeros_like(p[:,4:5][~obj]))\n",
        "    lb  = smooth_l1(p[:,:4][obj.expand_as(p[:,:4])], t[:,:4][obj.expand_as(t[:,:4])]) if obj.any() else torch.tensor(0.,device=p.device)\n",
        "    return Î»c*lb + Î»o*lo + Î»n*lno\n",
        "\n",
        "# =============== 5. TRAINING ===============\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model  = TinyYOLO().to(device)\n",
        "opt    = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
        "\n",
        "train_dl = DataLoader(FishDataset(train_imgs, augment=True), batch_size=8, shuffle=True,  collate_fn=collate)\n",
        "val_dl   = DataLoader(FishDataset(val_imgs),               batch_size=8, shuffle=False, collate_fn=collate)\n",
        "\n",
        "print(\"ğŸš€ Trainingâ€¦\")\n",
        "for epoch in range(1, 21):\n",
        "    model.train(); total=0\n",
        "    for imgs,tg in train_dl:\n",
        "        imgs, tg = imgs.to(device), tg.to(device)\n",
        "        loss = yolo_loss(model(imgs), tg)\n",
        "        opt.zero_grad(); loss.backward(); opt.step()\n",
        "        total += loss.item()\n",
        "    torch.cuda.empty_cache()\n",
        "    print(f\"Epoch {epoch:2d}/20 | loss {total/len(train_dl):.4f}\")\n",
        "\n",
        "# =============== 6. VALIDATION ===============\n",
        "print(\"ğŸ” Evaluatingâ€¦\")\n",
        "model.eval(); tp=fp=fn=0\n",
        "cell = IMG_SIZE / GRID_S\n",
        "with torch.no_grad():\n",
        "    for imgs, tg in val_dl:\n",
        "        preds = model(imgs.to(device)).cpu()\n",
        "        for b in range(preds.size(0)):\n",
        "            p = preds[b]; conf = torch.sigmoid(p[4])\n",
        "            idx = (conf>0.25).nonzero(as_tuple=False)\n",
        "            pred_boxes = []\n",
        "            for gj,gi in idx:\n",
        "                tx,ty,tw,th = p[0,gj,gi], p[1,gj,gi], p[2,gj,gi], p[3,gj,gi]\n",
        "                cx = (gi+torch.sigmoid(tx))*cell\n",
        "                cy = (gj+torch.sigmoid(ty))*cell\n",
        "                w  = torch.exp(tw).clamp(max=5)*cell\n",
        "                h  = torch.exp(th).clamp(max=5)*cell\n",
        "                pred_boxes.append([cx-w/2, cy-h/2, cx+w/2, cy+h/2])\n",
        "            pred_boxes = torch.tensor(pred_boxes) if pred_boxes else torch.zeros((0,4))\n",
        "\n",
        "            gt = []\n",
        "            for gj,gi in (tg[b,4]>0).nonzero(as_tuple=False):\n",
        "                tx,ty,tw,th = tg[b,0,gj,gi], tg[b,1,gj,gi], tg[b,2,gj,gi], tg[b,3,gj,gi]\n",
        "                cx=(gi+tx)*cell; cy=(gj+ty)*cell; w=tw*cell; h=th*cell\n",
        "                gt.append([cx-w/2, cy-h/2, cx+w/2, cy+h/2])\n",
        "            gt = torch.tensor(gt) if gt else torch.zeros((0,4))\n",
        "\n",
        "            if pred_boxes.numel()==0: fn += len(gt); continue\n",
        "            if gt.numel()==0: fp += len(pred_boxes); continue\n",
        "            iou = box_iou(pred_boxes, gt)\n",
        "            m,_ = iou.max(1)\n",
        "            tp += (m>=0.5).sum().item()\n",
        "            fp += (m<0.5).sum().item()\n",
        "            fn += gt.size(0) - (m>=0.5).sum().item()\n",
        "\n",
        "prec = tp/(tp+fp+1e-8); rec = tp/(tp+fn+1e-8)\n",
        "f1   = 2*prec*rec/(prec+rec+1e-8)\n",
        "print(f\"âœ… Precision={prec:.3f} Recall={rec:.3f} F1={f1:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pTK_JBkhjPpN",
        "outputId": "c3ac5e99-056e-47c7-fab2-da6d8079c2e7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â¬‡ï¸  Downloading dataset (ĞºĞµÑˆĞ¸Ñ€ÑƒĞµÑ‚ÑÑ)â€¦\n",
            "Using data.yaml: /kaggle/input/fish-detection/data.yaml\n",
            "Train dir: /kaggle/input/fish-detection/train/images\n",
            "Val dir  : /kaggle/input/fish-detection/valid/images\n",
            "ğŸš€ Trainingâ€¦\n",
            "Epoch  1/20 | loss 0.1487\n",
            "Epoch  2/20 | loss 0.0208\n",
            "Epoch  3/20 | loss 0.0086\n",
            "Epoch  4/20 | loss 0.0051\n",
            "Epoch  5/20 | loss 0.0035\n",
            "Epoch  6/20 | loss 0.0025\n",
            "Epoch  7/20 | loss 0.0020\n",
            "Epoch  8/20 | loss 0.0016\n",
            "Epoch  9/20 | loss 0.0013\n",
            "Epoch 10/20 | loss 0.0011\n",
            "Epoch 11/20 | loss 0.0009\n",
            "Epoch 12/20 | loss 0.0008\n",
            "Epoch 13/20 | loss 0.0007\n",
            "Epoch 14/20 | loss 0.0006\n",
            "Epoch 15/20 | loss 0.0005\n",
            "Epoch 16/20 | loss 0.0005\n",
            "Epoch 17/20 | loss 0.0004\n",
            "Epoch 18/20 | loss 0.0004\n",
            "Epoch 19/20 | loss 0.0003\n",
            "Epoch 20/20 | loss 0.0003\n",
            "ğŸ” Evaluatingâ€¦\n",
            "âœ… Precision=0.000 Recall=0.000 F1=0.000\n"
          ]
        }
      ]
    }
  ]
}